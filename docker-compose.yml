version: '3.9'

services:
  nova:
    image: nikolaik/python-nodejs:python3.11-nodejs18
    container_name: nova
    hostname: nova
    build:
      context: .
      dockerfile: ./assets/docker/Dockerfile
    ports:
      - 8080:8080
    expose:
      - 8080
    volumes:
      - .:/var/www/Nova-Platform
    depends_on:
      - nova-qdrant
      # - nova-llamacpp
    networks:
      - novaNetwork
  
  nova-qdrant:
    image: qdrant/qdrant:latest
    container_name: nova-db
    hostname: nova-db
    restart: always
    ports:
      - 6333:6333
      - 6334:6334
    expose:
      - 6333
      - 6334
      - 6335
    volumes:
      - ./assets/qdrant_data:/qdrant/storage
    networks:
      - novaNetwork
  
  chroma:
    image: chromadb/chroma:latest
    build:
      context: .
      dockerfile: ./assets/docker/Dockerfile.chroma
    volumes:
      - ./assets/chroma:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - PERSIST_DIRECTORY=/chroma/chroma
      - ANONYMIZED_TELEMETRY=${ANONYMIZED_TELEMETRY:-TRUE}
    ports:
      - 8000:8000
    expose:
      - 8000
    networks:
      - novaNetwork
    healthcheck: 
        test: curl localhost:8000/api/v1/heartbeat || exit 1
        interval: 10s
        retries: 2
        start_period: 5s
        timeout: 10s
  
  # Disable llama.cpp via Docker for now due to performance issues
  # nova-llamacpp:
  #   image: ghcr.io/ggerganov/llama.cpp:server
  #   container_name: nova-llamacpp
  #   hostname: nova-llamacpp
  #   command: >
  #     -m data/models/text/mistral-7b/mistral-7b-v0.1.Q2_K.gguf
  #     --port 8000
  #     --host 0.0.0.0
  #     --threads 20
  #     --mlock
  #     --keep -1
  #     --flash-attn
  #     --cont-batching
  #   ports:
  #     - 8000:8000
  #   expose:
  #     - 8000
  #   volumes:
  #     - ./assets:/data
  #   restart: unless-stopped
  #   networks:
  #     - novaNetwork

networks:
  novaNetwork:
    driver: bridge